# 第五章 神经网络

这一章的习题都放在[neural_networks](../neural_networks/)目录下, 目录结构如下:

```bash

```

## 5.1 试述线性函数$f(x) = w^Tx$用作神经元激活函数的缺陷.

答: 线性函数的线下叠加仍然是线性函数, 而理想的激活函数应具有跃迁或者阶跃的特性.

## 5.2 试述使用 图5.2(b) 的激活函数的神经元与对率回归的联系.

答: 对数几率回归(Logistic Regression), 简称对率回归. 用来将空间上的向量映射为0或1, 也就是用于做二分类. 单位阶跃函数是最理想的一种, 但因为不连续, 所以不是单调可微函数. 而$sigmoid$函数是单位阶跃函数的理想替代, 在一定程度上近似单位阶跃函数，并且单调可微，是任意阶可导的凸函数.  使用 $Sigmoid$ 激活函数，每个神经元几乎和对率回归相同，只不过对率回归在 $sigmoid(x)>0.5$ 时输出为1，而神经元直接输出 $sigmoid(x)$ 。

## 5.3 对于图5.7中的$v_{ih}$, 试推导出BP算法中的更新公式(5.13).

这题暂且摆烂了, 真的不是很擅长公式推导.  
解题的思路是, 原书P102-P103给出了以图5.7中隐层到输出层的连接权$w_{hj}$为例的推导过程, 输入层到隐层的连接权$v_{ih}$的推导过程可以照葫芦画瓢完成推导.

## 5.4 试述式(5.6)中学习率的取值对神经网络训练的影响

答: 学习率$\eta$类似于梯度下降过程的步长, 太大会导致误差函数在局部最低点来回震荡, 无法收敛. 太小则学习效率太低收敛过慢, 而且容易陷入局部最低点, 跳不出去.

## 5.5 试编程实现标准BP算法和累积BP算法, 在西瓜数据集3.0 上分别用这两个算法训练一个单隐层网络, 并进行比较.
