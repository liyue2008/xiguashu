# 第五章 神经网络

这一章的习题都放在[neural_networks](../neural_networks/)目录下, 目录结构如下:

```bash
$ tree neural_networks
neural_networks
├── __init__.py
├── accumulated_back_propagation.py  # 累积BP算法实现
├── back_propagation.py # 标准BP算法实现
├── dynamic_learning_rate.py # 动态学习率标准BP算法的实现
└── neural_networks_base.py # 神经网络数据模型、预测算法、均方差和累积误差的实现
```

## 5.1 试述线性函数$f(x) = w^Tx$用作神经元激活函数的缺陷.

答: 线性函数的线下叠加仍然是线性函数, 而理想的激活函数应具有跃迁或者阶跃的特性.

## 5.2 试述使用 图5.2(b) 的激活函数的神经元与对率回归的联系.

答: 对数几率回归(Logistic Regression), 简称对率回归. 用来将空间上的向量映射为0或1, 也就是用于做二分类. 单位阶跃函数是最理想的一种, 但因为不连续, 所以不是单调可微函数. 而$sigmoid$函数是单位阶跃函数的理想替代, 在一定程度上近似单位阶跃函数, 并且单调可微, 是任意阶可导的凸函数.  使用 $Sigmoid$ 激活函数, 每个神经元几乎和对率回归相同, 只不过对率回归在$sigmoid(x)>0.5$时输出为1, 而神经元直接输出$sigmoid(x)$。

## 5.3 对于图5.7中的$v_{ih}$, 试推导出BP算法中的更新公式(5.13).

这题暂且摆烂了, 真的不是很擅长公式推导.  
解题的思路是, 原书P102-P103给出了以图5.7中隐层到输出层的连接权$w_{hj}$为例的推导过程, 输入层到隐层的连接权$v_{ih}$的推导过程可以照葫芦画瓢完成推导.

## 5.4 试述式(5.6)中学习率的取值对神经网络训练的影响

答: 学习率$\eta$类似于梯度下降过程的步长, 太大会导致误差函数在局部最低点来回震荡, 无法收敛. 太小则学习效率太低收敛过慢, 而且容易陷入局部最低点, 跳不出去.

## 5.5 试编程实现标准BP算法和累积BP算法, 在西瓜数据集3.0 上分别用这两个算法训练一个单隐层网络, 并进行比较.

### 标准BP算法

标准BP算法在西瓜数据集3.0上以学习率$\eta=0.1$训练2000轮：

```bash
$ python3 -m neural_networks.back_propagation
输入 - 训练集:
       密度    含糖率 好瓜
0   0.697  0.460  是
1   0.774  0.376  是
2   0.634  0.264  是
3   0.608  0.318  是
4   0.556  0.215  是
5   0.403  0.237  是
6   0.481  0.149  是
7   0.437  0.211  是
8   0.666  0.091  否
9   0.243  0.267  否
10  0.245  0.057  否
11  0.343  0.099  否
12  0.639  0.161  否
13  0.657  0.198  否
14  0.360  0.370  否
15  0.593  0.042  否
16  0.719  0.103  否
输入 - 配置: {'stop_epoch': 2000, 'learning_rate': 0.1}
开始训练神经网络.
.........................................................................................................................................................................................................
训练完成, 耗时: 14.05s, 轮次: 2000, 累计误差: 0.170769.
输出-神经网络:
第2层2个神经元(阈值, 连接权值):
[ 4.327 -4.087]
[[ 1.582 -0.586]
 [ 5.523 -5.731]]
第1层2个神经元(阈值, 连接权值):
[1.466 1.602]
[[0.773 0.634]
 [2.425 9.548]]
第0层2个神经元(阈值, 连接权值):
[0 0]
```

### 累积BP算法

累积BP算法在西瓜数据集3.0上训练20轮：

```bash
$ python3 -m neural_networks.accumulated_back_propagation
输入 - 训练集:
       密度    含糖率 好瓜
0   0.697  0.460  是
1   0.774  0.376  是
2   0.634  0.264  是
3   0.608  0.318  是
4   0.556  0.215  是
5   0.403  0.237  是
6   0.481  0.149  是
7   0.437  0.211  是
8   0.666  0.091  否
9   0.243  0.267  否
10  0.245  0.057  否
11  0.343  0.099  否
12  0.639  0.161  否
13  0.657  0.198  否
14  0.360  0.370  否
15  0.593  0.042  否
16  0.719  0.103  否
输入 - 配置: {'stop_epoch': 2000, 'learning_rate': 0.1}
开始训练神经网络.
.........................................................................................................................................................................................................
训练完成, 耗时: 14.50s, 轮次: 2000, 累计误差: 0.248269.
输出-神经网络:
第2层2个神经元(阈值, 连接权值):
[0.287 0.723]
[[0.269 0.711]
 [0.807 0.817]]
第1层2个神经元(阈值, 连接权值):
[0.4   0.854]
[[0.556 0.029]
 [0.719 0.266]]
第0层2个神经元(阈值, 连接权值):
[0 0]
```

### 标准BP算法和累积BP算法比较

标准BP算法每次针对单个训练样例更新权值与阈值，参数更新频繁, 不同样例可能抵消, 需要多次迭代；累积BP算法其优化目标是最小化整个训练集上的累计误差读取整个训练集一遍才对参数进行更新, 参数更新频率较低。在很多任务中, 累计误差下降到一定程度后, 进一步下降会非常缓慢, 这时标准BP算法往往会获得较好的解, 尤其当训练集非常大时效果更明显.

以本题的例子，在西瓜数据集3.0，大约需要训练1000-2000轮可以看出二种算法收敛速度有明显差距.

### 修改训练参数

编辑[accumulated_back_propagation.py](../neural_networks/accumulated_back_propagation.py)和[back_propagation.py](../neural_networks/back_propagation.py)的main方法可以修改训练参数:

```python
config = {CONST_CONFIG_KEY_STOP_EPOCH: 20} # 停止条件是训练20轮
leaning_rate = 0.1 # 学习率
```

## 5.6 试设计一个BP改进算法，能通过动态调整学习率显著提升收敛速度. 编程实现该算法，并选择两个[UCI数据集](http://archive.ics.uci.edu/ml/)与标准BP算法进行实验比较

增大学习率可以提高收敛速度，但过高的学习率容易导致网络的错误率收敛到一定程度后反复震荡，这时就需要进一步降低学习率提高精度.  
这里实现一个最简单的动态调整学习率算法, 固定每n次训练, 将学习率乘以一个[0, 1)的系数, 以降低学习率.  
使用[UCI Iris Data Set](http://archive.ics.uci.edu/ml/datasets/Iris)(其中包含2个训练集: iris和bezdekIris)作为训练集, 分别在2个训练集上, 采用标准BP算法, 比较固定学习率和动态学习率的训练效果.

训练相关配置参数如下:

* 学习率(对于动态调整学习算法来说, 就是初始学习率): 0.2(learning_rate).
* 停止条件: 累计误差小于0.02(cumulative_error) 或者 训练轮次达到1000(stop_epoch)次(此时累积误差来回震荡无法继续收敛, 再继续训练下去已无意义).
* 动铁学习率参数: 每10(learning_rate_epoch)次训练, 将学习率乘以系数0.93(learning_rate_gamma), 以降低学习率.

训练对比效果如下表:

训练集 | 学习率算法 | 耗时(秒) | 轮次 | 累计误差
--|--|--|--|--
iris | 固定 | 93.69 | 896 | 0.019889
iris | 动态 | 45.69 | 441 | 0.019967
bezdekIris | 固定 | 106.11 | 1000 | 0.034754
bezdekIris | 动态 | 106.36 | 1000 | 0.129733

因为动态学习率的参数(learning_rate, learning_rate_epoch, learning_rate_gamma)是作者在iris数据集上多次手工调整后的较优配置, 可以看到在iris数据集上比固定学习率有更快的收敛速度. 但相同的参数在bezdekIris数据集上, 收敛速度反而不如固定学习率. 因此可以得出, 动态学习率参数对收敛效果影响非常大, 合适的参数的动态学习率在收敛速度上显著优于固定学习率, 不合适的参数反而不如固定学习率.

另外一个现象是, 固定学习率BP算法当累积误差收敛到一定程度后, 将发生震荡(overshoot), 不再收敛; 而动态学习率BP算法, 理论上可以通过降低学习率缓解震荡, 让累积误差随着训练轮次持续收敛. 但实践中, 动态学习率的调整算法和相关参数很关键, **学习率必须与当前训练情况匹配**. 学习率降低的太慢也会发生震荡导致收敛速度变慢, 学习率降低的太快所需的训练轮次大幅增加也会导致收敛速度变慢.

以下是程序执行情况:

```bash
$ python3 -m neural_networks.dynamic_learning_rate       
训练集: iris, 固定学习率.
输入 - 训练集:
     sepal length  sepal width  petal length  petal width           class
0             5.1          3.5           1.4          0.2     Iris-setosa
1             4.9          3.0           1.4          0.2     Iris-setosa
2             4.7          3.2           1.3          0.2     Iris-setosa
3             4.6          3.1           1.5          0.2     Iris-setosa
4             5.0          3.6           1.4          0.2     Iris-setosa
..            ...          ...           ...          ...             ...
145           6.7          3.0           5.2          2.3  Iris-virginica
146           6.3          2.5           5.0          1.9  Iris-virginica
147           6.5          3.0           5.2          2.0  Iris-virginica
148           6.2          3.4           5.4          2.3  Iris-virginica
149           5.9          3.0           5.1          1.8  Iris-virginica

[150 rows x 5 columns]
输入 - 配置: {'cumulative_error': 0.02, 'stop_epoch': 1000, 'learning_rate': 0.2, 'learning_rate_epoch': 10, 'learning_rate_gamma': 0.93}
开始训练神经网络.
...........................................................................................
训练完成, 耗时: 93.69s, 轮次: 896, 累计误差: 0.019889.
输出-神经网络:
第2层3个神经元(阈值, 连接权值):
[ 1.928 -0.69   1.714]
[[-3.465 -8.441  8.988]
 [ 7.653 -7.489 -1.892]
 [-1.11   1.812 -1.696]
 [-0.842  1.261 -1.048]]
第1层4个神经元(阈值, 连接权值):
[-0.048 11.567  0.068  0.098]
[[ 3.211e-01 -1.115e+01  9.653e-01  4.019e-01]
 [ 2.712e+00 -1.082e+01  6.812e-01  7.945e-03]
 [-3.412e+00  1.659e+01  9.067e-01  6.917e-01]
 [-1.507e+00  1.950e+01  2.522e-01  5.927e-01]]
第0层4个神经元(阈值, 连接权值):
[0 0 0 0]

训练集: iris, 动态学习率.
输入 - 训练集:
     sepal length  sepal width  petal length  petal width           class
0             5.1          3.5           1.4          0.2     Iris-setosa
1             4.9          3.0           1.4          0.2     Iris-setosa
2             4.7          3.2           1.3          0.2     Iris-setosa
3             4.6          3.1           1.5          0.2     Iris-setosa
4             5.0          3.6           1.4          0.2     Iris-setosa
..            ...          ...           ...          ...             ...
145           6.7          3.0           5.2          2.3  Iris-virginica
146           6.3          2.5           5.0          1.9  Iris-virginica
147           6.5          3.0           5.2          2.0  Iris-virginica
148           6.2          3.4           5.4          2.3  Iris-virginica
149           5.9          3.0           5.1          1.8  Iris-virginica

[150 rows x 5 columns]
输入 - 配置: {'cumulative_error': 0.02, 'stop_epoch': 1000, 'learning_rate': 0.2, 'learning_rate_epoch': 10, 'learning_rate_gamma': 0.93}
开始训练神经网络.
..............................................
训练完成, 耗时: 45.69s, 轮次: 441, 累计误差: 0.019967.
输出-神经网络:
第2层3个神经元(阈值, 连接权值):
[ 2.254  1.332 -0.3  ]
[[ 0.836  6.745 -6.73 ]
 [ 7.724 -6.989 -5.196]
 [-1.548 -1.185  1.786]
 [-1.033 -0.691  1.329]]
第1层4个神经元(阈值, 连接权值):
[0.569 2.759 0.067 0.901]
[[-0.799 -3.525  0.977  0.792]
 [-1.666 -3.551  0.883  0.637]
 [ 3.172  5.145  0.529  0.625]
 [ 1.856  6.166  0.092  0.564]]
第0层4个神经元(阈值, 连接权值):
[0 0 0 0]

训练集: bezdekIris, 固定学习率.
输入 - 训练集:
     sepal length  sepal width  petal length  petal width           class
0             5.1          3.5           1.4          0.2     Iris-setosa
1             4.9          3.0           1.4          0.2     Iris-setosa
2             4.7          3.2           1.3          0.2     Iris-setosa
3             4.6          3.1           1.5          0.2     Iris-setosa
4             5.0          3.6           1.4          0.2     Iris-setosa
..            ...          ...           ...          ...             ...
145           6.7          3.0           5.2          2.3  Iris-virginica
146           6.3          2.5           5.0          1.9  Iris-virginica
147           6.5          3.0           5.2          2.0  Iris-virginica
148           6.2          3.4           5.4          2.3  Iris-virginica
149           5.9          3.0           5.1          1.8  Iris-virginica

[150 rows x 5 columns]
输入 - 配置: {'cumulative_error': 0.02, 'stop_epoch': 1000, 'learning_rate': 0.2, 'learning_rate_epoch': 10, 'learning_rate_gamma': 0.93}
开始训练神经网络.
.....................................................................................................
训练完成, 耗时: 106.11s, 轮次: 1000, 累计误差: 0.034754.
输出-神经网络:
第2层3个神经元(阈值, 连接权值):
[ 2.384  1.892 -0.568]
[[ 0.887  8.566 -9.072]
 [-1.304 -1.202  2.161]
 [-1.493 -1.383  1.803]
 [ 7.795 -7.537 -3.761]]
第1层4个神经元(阈值, 连接权值):
[ 1.269  0.433  0.248 12.142]
[[ -0.585   1.041   0.752 -11.357]
 [ -2.126   0.418   0.96  -10.482]
 [  3.659   0.571   0.304  17.034]
 [  1.744   0.94    0.798  20.501]]
第0层4个神经元(阈值, 连接权值):
[0 0 0 0]

训练集: bezdekIris, 动态学习率.
输入 - 训练集:
     sepal length  sepal width  petal length  petal width           class
0             5.1          3.5           1.4          0.2     Iris-setosa
1             4.9          3.0           1.4          0.2     Iris-setosa
2             4.7          3.2           1.3          0.2     Iris-setosa
3             4.6          3.1           1.5          0.2     Iris-setosa
4             5.0          3.6           1.4          0.2     Iris-setosa
..            ...          ...           ...          ...             ...
145           6.7          3.0           5.2          2.3  Iris-virginica
146           6.3          2.5           5.0          1.9  Iris-virginica
147           6.5          3.0           5.2          2.0  Iris-virginica
148           6.2          3.4           5.4          2.3  Iris-virginica
149           5.9          3.0           5.1          1.8  Iris-virginica

[150 rows x 5 columns]
输入 - 配置: {'cumulative_error': 0.02, 'stop_epoch': 1000, 'learning_rate': 0.2, 'learning_rate_epoch': 10, 'learning_rate_gamma': 0.93}
开始训练神经网络.
.....................................................................................................
训练完成, 耗时: 106.36s, 轮次: 1000, 累计误差: 0.129733.
输出-神经网络:
第2层3个神经元(阈值, 连接权值):
[2.581 1.161 0.609]
[[ -1.586   0.475   0.699]
 [ -0.822  -0.144   0.802]
 [  9.557  -0.17  -11.982]
 [ -1.57    0.216   1.251]]
第1层4个神经元(阈值, 连接权值):
[0.957 0.235 0.957 0.94 ]
[[ 0.959  0.538 -0.607  0.689]
 [ 0.957  1.167 -1.074  1.091]
 [ 0.824  0.18   1.064  0.977]
 [ 0.444  0.668  2.137  0.526]]
第0层4个神经元(阈值, 连接权值):
[0 0 0 0]
```

### 如何调整动态学习率相关参数?

在[dynamic_learning_rate.py](../neural_networks/dynamic_learning_rate.py)中的配置变量config增加如下配置:

```python
CONST_CONFIG_KEY_VERBOSE: True, # 打印详细训练信息
```

可以打印每轮的累计误差和变化以及当前学习率, 然后观察收敛速度和是否发生震荡来调整学习率相关参数.

```bash
轮次: 累积误差 (较上次变化) 学习率
0: 0.732081 (+0.000000) 0.200000
1: 0.472344 (-0.259737) 0.200000
2: 0.475879 (+0.003535) 0.200000
...
```

## 5.8 根据式(5.18)和(5.19),试构造一个能解决异或问题的单层RBF神经网络.
