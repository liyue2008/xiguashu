# 第四章 决策树

这一章的习题都放在[decision_tree](../decision_tree/)目录下, 目录结构如下:

```bash
(xiguashu)  ~/temp/xiguashu $ tree decision_tree
decision_tree
├── __init__.py
├── decision_tree_base.py # 决策树生成基本算法的实现, 对应书中P74 图 4.2
├── decision_tree_queue.py # 习题4.8：使用队列实现的决策树生成算法, 广度搜索优先, 支持限制树的最大节点数。
├── decision_tree_stack.py # 习题4.7：使用栈实现的决策树生成算法, 深度搜索优先, 支持限制树的最大深度。
├── gain.py # 对应习题4.3, 用信息增益来进行决策树的最优划分属性选择, 采用二分法(bi-partition)对连续属性进行处理的决策树生成算法实现.
├── gini.py # 习题4.4 采用基尼指数进行划分选择的基本决策树算法, 不支持剪枝。
└── pruning.py # 习题4.4 采用基尼指数进行划分选择的决策树算法, 支持预剪枝和后剪枝。
```

## 4.3 试编程实现基于信息嫡进行划分选择的决策树算法, 并为表4.3中数据生成一棵决策树.  

用信息增益来进行决策树的最优划分属性选择, 采用二分法(bi-partition)对连续属性进行处理的决策树生成算法实现.

```bash
(xiguashu)  ~/workspace/xiguashu $ python3 -m decision_tree.gain   
输入-数据集 D:
DataSet: label_name=好瓜, samples(17):
    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率 好瓜
编号                                         
1   青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460  是
2   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376  是
3   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264  是
4   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318  是
5   浅白  蜷缩  浊响  清晰  凹陷  硬滑  0.556  0.215  是
6   青绿  稍蜷  浊响  清晰  稍凹  软粘  0.403  0.237  是
7   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149  是
8   乌黑  稍蜷  浊响  清晰  稍凹  硬滑  0.437  0.211  是
9   乌黑  稍蜷  沉闷  稍糊  稍凹  硬滑  0.666  0.091  否
10  青绿  硬挺  清脆  清晰  平坦  软粘  0.243  0.267  否
11  浅白  硬挺  清脆  模糊  平坦  硬滑  0.245  0.057  否
12  浅白  蜷缩  浊响  模糊  平坦  软粘  0.343  0.099  否
13  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161  否
14  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198  否
15  乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370  否
16  浅白  蜷缩  浊响  模糊  平坦  硬滑  0.593  0.042  否
17  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103  否

输入-属性集 A:
{Continuous attribute: name=密度, Discrete attribute: name=色泽, values={'乌黑', '青绿', '浅白'}, Continuous attribute: name=含糖率, Discrete attribute: name=敲声, values={'清脆', '浊响', '沉闷'}, Discrete attribute: name=纹理, values={'清晰', '模糊', '稍糊'}, Discrete attribute: name=根蒂, values={'蜷缩', '稍蜷', '硬挺'}, Discrete attribute: name=脐部, values={'稍凹', '凹陷', '平坦'}, Discrete attribute: name=触感, values={'软粘', '硬滑'}}

输出-决策树:
Classify(分类属性): 纹理, children(3):
|--纹理=清晰: Classify(分类属性): 密度, children(2):
  |--密度=0.382-: Label(标记值): 否
  |--密度=0.382+: Label(标记值): 是
|--纹理=模糊: Label(标记值): 否
|--纹理=稍糊: Classify(分类属性): 密度, children(2):
  |--密度=0.560-: Label(标记值): 是
  |--密度=0.560+: Label(标记值): 否
```

## 4.4 试编程实现基于基尼指数进行划分选择的决策树算法, 为表4.2中数据生成预剪枝、后剪枝决策树, 并与未剪枝决策树进行比较

使用基尼指数进行划分选择, 依次生成三棵决策树：未剪枝、预剪枝和后剪枝, 并计算这三棵树在验证集上的精度。

```bash
(xiguashu)  ~/workspace/xiguashu $ python3 -m decision_tree.pruning
输入-训练集:
DataSet: label_name=好瓜, samples(10):
    色泽  根蒂  敲声  纹理  脐部  触感 好瓜
编号
1   青绿  蜷缩  浊响  清晰  凹陷  硬滑  是
2   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  是
3   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  是
6   青绿  稍蜷  浊响  清晰  稍凹  软粘  是
7   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  是
10  青绿  硬挺  清脆  清晰  平坦  软粘  否
14  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  否
15  乌黑  稍蜷  浊响  清晰  稍凹  软粘  否
16  浅白  蜷缩  浊响  模糊  平坦  硬滑  否
17  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  否

输入-验证集:
DataSet: label_name=好瓜, samples(7):
    色泽  根蒂  敲声  纹理  脐部  触感 好瓜
编号
4   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  是
5   浅白  蜷缩  浊响  清晰  凹陷  硬滑  是
8   乌黑  稍蜷  浊响  清晰  稍凹  硬滑  是
9   乌黑  稍蜷  沉闷  稍糊  稍凹  硬滑  否
11  浅白  硬挺  清脆  模糊  平坦  硬滑  否
12  浅白  蜷缩  浊响  模糊  平坦  软粘  否
13  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  否

输入-属性集 A:
{Discrete attribute: name=脐部, values={'平坦', '凹陷', '稍凹'}, Discrete attribute: name=触感, values={'硬滑', '软粘'}, Discrete attribute: name=色泽, values={'浅白', '乌黑', '青绿'}, Discrete attribute: name=敲声, values={'浊响', '清脆', '沉闷'}, Discrete attribute: name=根蒂, values={'蜷缩', '硬挺', '稍蜷'}, Discrete attribute: name=纹理, values={'稍糊', '模糊', '清晰'}}

输出-未剪枝的决策树, 精度 = 57.1%.
Classify(分类属性): 脐部, children(3):
|--脐部=平坦: Label(标记值): 否
|--脐部=凹陷: Classify(分类属性): 色泽, children(3):
  |--色泽=浅白: Label(标记值): 否
  |--色泽=乌黑: Label(标记值): 是
  |--色泽=青绿: Label(标记值): 是
|--脐部=稍凹: Classify(分类属性): 触感, children(2):
  |--触感=硬滑: Label(标记值): 否
  |--触感=软粘: Classify(分类属性): 色泽, children(3):
    |--色泽=浅白: Label(标记值): 是
    |--色泽=乌黑: Classify(分类属性): 纹理, children(3):
      |--纹理=稍糊: Label(标记值): 是
      |--纹理=模糊: Label(标记值): 是
      |--纹理=清晰: Label(标记值): 否
    |--色泽=青绿: Label(标记值): 是
==========

输出-预剪枝的决策树, 精度 = 71.4%.
Classify(分类属性): 脐部, children(3):
|--脐部=平坦: Label(标记值): 否
|--脐部=凹陷: Label(标记值): 是
|--脐部=稍凹: Label(标记值): 是
==========

输出-后剪枝的决策树, 精度 = 57.1%.
Classify(分类属性): 脐部, children(3):
|--脐部=平坦: Label(标记值): 否
|--脐部=凹陷: Classify(分类属性): 色泽, children(3):
  |--色泽=浅白: Label(标记值): 否
  |--色泽=乌黑: Label(标记值): 是
  |--色泽=青绿: Label(标记值): 是
|--脐部=稍凹: Classify(分类属性): 触感, children(2):
  |--触感=硬滑: Label(标记值): 否
  |--触感=软粘: Classify(分类属性): 色泽, children(3):
    |--色泽=浅白: Label(标记值): 是
    |--色泽=乌黑: Classify(分类属性): 纹理, children(3):
      |--纹理=稍糊: Label(标记值): 是
      |--纹理=模糊: Label(标记值): 是
      |--纹理=清晰: Label(标记值): 否
    |--色泽=青绿: Label(标记值): 是
```

## 4.7 图 4.2 是一个递归算法, 若面临巨量数据, 则决策树的层数会很深, 是用递归方法易导致栈溢出, 试使用队列数据结构, 以参数MaxDepth控制树的最大深度, 写出与图4.2等价、但不使用递归的决策树生成算法。

题目中要求“队列数据结构”实现等价的决策树生成算法, 如果严格要求使用“先进先出队列”, 我没想到如何实现。
这里给出的是使用双向队列（deque）数据结构, 模拟栈（stack, “后进先出”）实现的等价算法。
基本思想与图4.2的递归生成算法一样, 采用深度遍历优先策略, 只是用栈代替了递归。

```bash
(xiguashu)  ~/workspace/xiguashu $ python3 -m decision_tree.decision_tree_stack
输入-数据集 D:
DataSet: label_name=好瓜, samples(17):
    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率 好瓜
编号
1   青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460  是
2   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376  是
3   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264  是
4   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318  是
5   浅白  蜷缩  浊响  清晰  凹陷  硬滑  0.556  0.215  是
6   青绿  稍蜷  浊响  清晰  稍凹  软粘  0.403  0.237  是
7   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149  是
8   乌黑  稍蜷  浊响  清晰  稍凹  硬滑  0.437  0.211  是
9   乌黑  稍蜷  沉闷  稍糊  稍凹  硬滑  0.666  0.091  否
10  青绿  硬挺  清脆  清晰  平坦  软粘  0.243  0.267  否
11  浅白  硬挺  清脆  模糊  平坦  硬滑  0.245  0.057  否
12  浅白  蜷缩  浊响  模糊  平坦  软粘  0.343  0.099  否
13  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161  否
14  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198  否
15  乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370  否
16  浅白  蜷缩  浊响  模糊  平坦  硬滑  0.593  0.042  否
17  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103  否

输入-属性集 A:
{Discrete attribute: name=触感, values={'硬滑', '软粘'}, Continuous attribute: name=含糖率, Discrete attribute: name=色泽, values={'浅白', '青绿', '乌黑'}, Discrete attribute: name=敲声, values={'沉闷', '清脆', '浊响'}, Discrete attribute: name=脐部, values={'凹陷', '稍凹', '平坦'}, Discrete attribute: name=根蒂, values={'硬挺', '蜷缩', '稍蜷'}, Discrete attribute: name=纹理, values={'清晰', '稍糊', '模糊'}, Continuous attribute: name=密度}

输入-最大深度：2.

输出-决策树:
Classify(分类属性): 纹理, children(3):
|--纹理=模糊: Label(标记值): 否
|--纹理=清晰: Label(标记值): 是
|--纹理=稍糊: Label(标记值): 否
```

## 4.8 试将决策树生成的深度优先搜索过程修改为广度优先搜索, 以参数 MaxNode控制树的最大结点数, 将题4.7中基于队列的决策树算法进行改写.对比题4.7中的算法, 试析哪种方式更易于控制决策树所需存储不超出内存.

显然, 最大节点数方法更易于控制决策树生成算法的内存占用。原因是每个节点的内存大小几乎是相同的, 所以可以精确的计算出树的内存占用：
> 树的内存占用 = 单节点内存占用 * 树的节点数量

而通过控制树的最大深度来限制内存占用, 因为每层节点数量并不固定, 所以精度较差。

基本思想是: 广度遍历优先, 逐层生成树节点.

1. 构造根节点, 将根节点和对应的训练集, 属性集构造成一个对象入队.
2. while 队列不为空:
   1. 从队中取出一个节点
   2. 按照 4.2 中的算法判断当前节点是否应是叶子节点:
   3. 如果是, 则按照算法生成叶子节点, 并将这些子节点的父节点设为当前节点;
   4. 如果否, 则按照算法生成子节点, 以及子节点对应的训练集和属性集, 构造成一个对象入队.
3. 返回根节点.

本例和习题4.3 采用同样信息嫡进行划分选择算法, 同样的训练集, 因此训练得到的决策树也是相同的.

```bash
(xiguashu)  ~/workspace/xiguashu $ python3 -m decision_tree.decision_tree_queue
输入-数据集 D:
DataSet: label_name=好瓜, samples(17):
    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率 好瓜
编号
1   青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460  是
2   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376  是
3   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264  是
4   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318  是
5   浅白  蜷缩  浊响  清晰  凹陷  硬滑  0.556  0.215  是
6   青绿  稍蜷  浊响  清晰  稍凹  软粘  0.403  0.237  是
7   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149  是
8   乌黑  稍蜷  浊响  清晰  稍凹  硬滑  0.437  0.211  是
9   乌黑  稍蜷  沉闷  稍糊  稍凹  硬滑  0.666  0.091  否
10  青绿  硬挺  清脆  清晰  平坦  软粘  0.243  0.267  否
11  浅白  硬挺  清脆  模糊  平坦  硬滑  0.245  0.057  否
12  浅白  蜷缩  浊响  模糊  平坦  软粘  0.343  0.099  否
13  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161  否
14  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198  否
15  乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370  否
16  浅白  蜷缩  浊响  模糊  平坦  硬滑  0.593  0.042  否
17  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103  否

输入-属性集 A:
{Continuous attribute: name=密度, Discrete attribute: name=触感, values={'硬滑', '软粘'}, Continuous attribute: name=含糖率, Discrete attribute: name=敲声, values={'沉闷', '浊响', '清脆'}, Discrete attribute: name=色泽, values={'青绿', '乌黑', '浅白'}, Discrete attribute: name=根蒂, values={'蜷缩', '硬挺', '稍蜷'}, Discrete attribute: name=脐部, values={'稍凹', '凹陷', '平坦'}, Discrete attribute: name=纹理, values={'稍糊', '模糊', '清晰'}}

输入-最大节点数：4.

输出-决策树:
Classify(分类属性): 纹理, children(3):
|--纹理=稍糊: Label(标记值): 否
|--纹理=模糊: Label(标记值): 否
|--纹理=清晰: Label(标记值): 是
```
